max_seq_length: 1024 # Max Sequence Length for tokenization 
dtype: null # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4_bit: False # Use 4bit quantization to reduce memory usage. Can be False.
strategy: 'l2m' # One of these ['cot', 'pot', 'l2m']
num_generation_rounds: 10 # How many samples to generate for each training datapoint
checkpoint_path: '/cluster/work/sachan/shivam/improving-prompting-strategies/model/knowledge_distillation/llama3-8b/gemma-2b-it-lora/combined/checkpoint-1100'
model_dir: '/cluster/work/sachan/shivam/improving-prompting-strategies/model/knowledge_distillation/llama3-8b/gemma-2b-it-lora/action-reward/' # Directory to save merged model - will be deleted when the program completes 
